{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022737d5-861a-426a-8b7d-58287cacf21d",
   "metadata": {},
   "source": [
    "# Funciones para realizar chunking (parsing parcial) de textos en Español"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515b806-df5f-48e7-a5e7-42de70719f63",
   "metadata": {},
   "source": [
    "La idea general:\n",
    "Encontrar “trozos” sintácticos del texto (sobre todo Frases Nominales, FN) usando dos enfoques:\n",
    "\n",
    "Con spaCy (doc.noun_chunks)\n",
    "\n",
    "Con NLTK + una gramática de expresiones regulares sobre etiquetas POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ab0ae1-1cc6-4263-b093-9c33ec32acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para importar el módulo con las funciones necesarias\n",
    "import sys\n",
    "sys.path.append(r\"D:\\Repos Github\\AnaliticaTextual-Ejemplos\") # la carpeta donde está el módulo\n",
    "\n",
    "import utils # el módulo -> utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01fc393-516f-4a12-ba38-bbf49be04c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser as RE_Parser\n",
    "# RegexpParser: clase de NLTK que permite definir gramáticas de chunking con expresiones regulares sobre etiquetas POS.\n",
    "import es_core_news_sm # procesador de lenguaje con segmentación de frases, tokenización, lematización, etiquetas POS y dependencias sintácticas. Carga el modelo en español de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ab56ae-d47a-4e9d-8a5e-60ed19121f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos modelo de Spacy\n",
    "nlp = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cbb1c83-694b-4cba-a5d1-d9bdfcda6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del corpus\n",
    "FILENAME=r'D:\\Repos Github\\AnaliticaTextual-Ejemplos\\CORPUS\\deportes\\d1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8766c8c0-9478-410c-a2cb-4a45ed5c50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = open(FILENAME, 'r', encoding='latin-1').read()\n",
    "# Aquí NO se trocea por frases, sino que se pasa el documento entero a las funciones de chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f19bad-1ccb-4534-a2a6-736bb44d31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun chunks con spaCy\n",
    "\n",
    "def Chunk_FN(texto):\n",
    "   doc = nlp(texto) #procesa texto con spacy\n",
    "   lista_FN = [chunk.text for chunk in doc.noun_chunks] \n",
    "    # doc.noun_chunks ->Es un generador de frases nominales que spaCy detecta automáticamente usando el árbol de dependencias.\n",
    "    # [chunk.text for chunk in doc.noun_chunks] ->Extrae solo el texto de cada chunk nominal.\n",
    "   return(lista_FN)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e275a6e-6fe3-49c7-bc75-1affabba34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking con NLTK + gramática\n",
    "\n",
    "def Chunk(texto):\n",
    "  tagged1 = utils.Etiquetar(texto). split() # -> lista de strings \"palabra/POS\"\n",
    "  tagged = [tuple(s.split('/')) for s in tagged1] # ransformar cada \"palabra/POS\" en una tupla (\"palabra\", \"POS\"), utilizamos \"/\" como elemento del split \n",
    "    #-> formato que espera NLTK para chunking: lista de tuplas (palabra, etiqueta_POS)\n",
    "  #print(tagged) para comprobar que el proceso funciona, lo qujito para no confundirme cuando imprima \n",
    "  # Definir un chunk del tipo Frase Nominal (FN). \n",
    "  # Ejemplo: articulo* nombre* adjetivo*\n",
    "  gramatica = '''                                                                                                              \n",
    "    FN:                                                                                                                    \n",
    "        {<DET>*(<PROPN|NOUN>)+<ADJ>*}\n",
    "  \n",
    "    '''\n",
    "# <DET>* → cero o más determinantes: rtículos, demostrativos, posesivos, etc. (“el”, “la”, “este”, “su”, …)\n",
    "# <PROPN|NOUN>)+ → uno o más:\n",
    "    #PROPN = nombre propio (“Messi”, “Madrid”, “España”, …)\n",
    "    #NOUN = nombre común (“delantero”, “equipo”, “partido”, …)\n",
    "# <ADJ>* → cero o más adjetivos: “argentino”, “local”, “grande”, “histórico”, …\n",
    "\n",
    "# Con esto atrapa secuencias tipo \"el delantero”, “el delantero argentino”, “este joven delantero argentino”, “Messi”, “Messi argentino zurdo”\n",
    "    \n",
    "    \n",
    "  chunker = RE_Parser(gramatica) # creas el analizador de chunks de NLTK\n",
    "  Arbol = chunker.parse(tagged) # Aquí es donde se aplica la gramática. Produce un árbol sintáctico “parcial” (un nltk.Tree) donde:\n",
    "    # algunos grupos de tokens están agrupados como FN\n",
    "    # el resto queda como tokens sueltos\n",
    "  matches =[] # Aquí guardarás todos los subárboles que sean frases nominales (FN).\n",
    "  for subarbol in Arbol.subtrees():\n",
    "        if subarbol.label() == 'FN': \n",
    "            matches.append(subarbol)\n",
    "  return(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215ca6da-1bf6-4b79-8161-7bcf8ea605fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Subarbol_a_Lista(Chunks):\n",
    "  lista =[]\n",
    "  for c in Chunks:\n",
    "    palabras = []\n",
    "    for (palabra,_pos) in c:\n",
    "        palabras.append(palabra) # sólo cogemos palabra, no pos\n",
    "    lista.append(' '.join(palabras))\n",
    "  return(lista)  \n",
    "# Así quedaría (ejemplo) -> [\"El delantero argentino\", \"los goles\", \"el equipo local\", ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e500c255-8e9a-4e9c-aba7-3be251c13db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = Chunk_FN(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9f2202-334b-4e2d-a1a6-f8c6dfd17a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El golfista chileno', 'la segunda jornada', 'una tarjeta', '+1', 'la parte final', 'certamen', 'que', 'Texas', ', Estados Unidos', 'Joaquín Niemann', 'lo', 'El golfista chileno', 'el corte', 'fin de semana', 'el Charles Schwab Challenge del PGA Tour', 'El chileno', 'día', 'muy parejo', 'Texas', ', con mejor rendimiento', 'la primera parte', ', pero con una segunda ronda', 'que', 'una tarjeta acumulada de +1', 'El talagantino', 'varias horas', 'los resultados', 'los otros rivales', 'el corte', '+2', 'competencia', 'Niemann', 'dos birdies', '(hoyos', 'dos bogeys', '(banderas', 'su registro', 'su participación', 'Chile', 'El sueco Jonas Blixt', 'como líder', '-9', 'cuatro birdies', '(hoyos 1, 2, 12 y 13)', 'un águila', 'la bandera']\n"
     ]
    }
   ],
   "source": [
    "print(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02251d53-65e6-43a9-a3d2-610849f7bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN2 = Subarbol_a_Lista(Chunk(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b553e6a-c295-4fbf-9be9-5e4ca0730693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El golfista chileno', 'jornada', 'una tarjeta', '+1', 'la parte final', 'certamen', 'Texas', 'Estados Unidos', 'Joaquín Niemann', 'El golfista chileno', 'el corte', 'fin', 'semana', 'el Charles Schwab Challenge', 'PGA Tour', 'El chileno', 'día', 'parejo', 'Texas', 'rendimiento', 'parte', 'ronda', 'una tarjeta acumulada', '+1', 'El talagantino', 'varias horas', 'los resultados', 'los otros rivales', 'el corte', '+2', 'competencia', 'Niemann', 'birdies', 'hoyos', 'bogeys', 'banderas', 'su registro', 'su participación', 'partir', 'horas', 'Chile', 'El sueco Jonas Blixt', 'líder', '-9', 'birdies', 'hoyos', 'un águila', 'la bandera']\n"
     ]
    }
   ],
   "source": [
    "print(FN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b3740-8d9b-4373-8d61-8417fd9c5c4a",
   "metadata": {},
   "source": [
    "Es más moderno usar FN. FN2 es un proceso manual. Atkinson lo pone aquí para comparar ambos sistemas. \n",
    "\n",
    "spaCy ya viene con el modelo entrenado para:\n",
    "\n",
    "detectar tokens\n",
    "\n",
    "etiquetar POS\n",
    "\n",
    "analizar dependencias\n",
    "\n",
    "y a partir de eso sacar automáticamente las Frases Nominales (FN)\n",
    "\n",
    "No tienes que:\n",
    "\n",
    "definir una gramática ({<DET>*(<PROPN|NOUN>)+<ADJ>*}),\n",
    "\n",
    "partir cadenas \"palabra/POS\",\n",
    "\n",
    "usar NLTK ni RegexpParser,\n",
    "\n",
    "ni pelearte con árboles.\n",
    "\n",
    "Tú solo cargas el modelo (es_core_news_sm) y le pasas texto.\n",
    "El “modelo” ya está ahí dentro; lo que no tienes que hacer es diseñar tú las reglas.\n",
    "\n",
    "Por eso:\n",
    "\n",
    "FN (spaCy) → enfoque moderno, sencillo, práctico.\n",
    "\n",
    "FN2 (NLTK + gramática) → enfoque clásico, didáctico, pero más engorroso y desfasado.\n",
    "\n",
    "Para todo lo que tú quieres aprender de Analítica Textual te vale de sobra con spaCy.\n",
    "Lo otro úsalo solo como “arqueología” para entender cómo se hacía antes.\n",
    "\n",
    "JFBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc04598-3d2f-42fa-b5d3-8bd9423bc9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (atkinson)",
   "language": "python",
   "name": "atkinson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
